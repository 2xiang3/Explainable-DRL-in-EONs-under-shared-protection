{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanha\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "import time as t\n",
    "import pandas as pd\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as MLP_PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy as MLP_DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from explainable.utils import evaluate_policy\n",
    "#from explainable.envs.deeprmsa_env import shortest_available_path_first_fit\n",
    "from explainable.dagger import DAgger_Policy\n",
    "stable_baselines3.__version__ # printing out stable_baselines version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EnvExpert(top_name, k, alg_name, base_log_dir='./tmp/', only_spectrum_obs = False, mean_service_holding_time=10):\n",
    "    topology_dir = '/topologies/demo/' +  top_name +f'_{k}.h5'\n",
    "    with open(f'..{topology_dir}', 'rb') as f:\n",
    "        topology = pickle.load(f)\n",
    "    assert k <= topology.graph['k_paths']\n",
    "    \n",
    "    node_request_probabilities = np.array([1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11])\n",
    "\n",
    "    env_args = dict(topology=topology, seed=10, \n",
    "                    allow_rejection=False, # the agent cannot proactively reject a request\n",
    "                    j=1, # consider only the first suitable spectrum block for the spectrum assignment\n",
    "                    mean_service_holding_time=mean_service_holding_time, # value is not set as in the paper to achieve comparable reward values\n",
    "                    episode_length=50, node_request_probabilities=node_request_probabilities, num_spectrum_resources = 358)\n",
    "\n",
    "    # Create log dir\n",
    "    log_dir = \"./tmp/deeprmsa-dqn-sbpp-agent-{}-cost239/\".format(mean_service_holding_time)\n",
    "    env = gym.make('DeepRMSA-v0', **env_args)\n",
    "\n",
    "    # logs will be saved in log_dir/monitor.csv\n",
    "    # in this case, on top of the usual monitored things, we also monitor service and bit rate blocking probabilities\n",
    "    env = Monitor(env, log_dir + 'testing', info_keywords=('episode_service_blocking_rate','bit_rate_blocking_rate','failure', 'episode_failure',\n",
    "                        'failure_slots','episode_failure_slots', \n",
    "                        'failure_disjointness','episode_failure_disjointness', 'failure_shared_disjointness',\n",
    "                        'episode_failure_shared_disjointness','shared_counter','episode_shared_counter', 'dpp_counter',\n",
    "                        'episode_dpp_counter','compactness', 'throughput', 'available_slots_working', 'available_slots_backup'))\n",
    "    \n",
    "    expert = DQN.load(log_dir +'best_model')\n",
    "        \n",
    "    return env, expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = 'DQN'\n",
    "top_name = 'cost239'\n",
    "k_path = 5\n",
    "traffics = [100, 200, 300, 400, 500]\n",
    "holding_time = [10, 20, 30, 40, 50]\n",
    "min_samples_splits = [2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "max_depth = 12\n",
    "n_eval_episodes = 2000\n",
    "use_heuristic_trainer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from explainable.dagger import DAgger\n",
    "from explainable.utils import collect_transitions\n",
    "\n",
    "start = t.time()\n",
    "env, expert = get_EnvExpert(top_name, k_path, alg_name, mean_service_holding_time=10)\n",
    "demostrations = collect_transitions(expert, env, 20000)\n",
    "end = t.time()\n",
    "timer = end - start\n",
    "timer = round(timer/60,2)\n",
    "\n",
    "print(f\"Load envrionment and collect transitions done with duration of {timer} minutes\")\n",
    "\n",
    "for min_samples_split in min_samples_splits: \n",
    "    start = t.time()\n",
    "    # Decision Trees Dagger Trainer:\n",
    "    tree_regr = tree.DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split) # depth is set only for visualization purposes\n",
    "    tree_dagger = DAgger(expert, tree_regr, env, demostrations.copy(), max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "\n",
    "    # Training the student policy:\n",
    "    tree_dagger.train(expert, env)\n",
    "    \n",
    "    # saving the student policy:\n",
    "    save_dir = \"./tmp_DT_max_depth/\" + f'cost239_{k_path}_{min_samples_split}/'\n",
    "    tree_dagger.policy.save(save_dir)\n",
    "    end = t.time()\n",
    "    timer = end - start\n",
    "    timer = round(timer/60,2)\n",
    "    print(f'done for DTClassifier with min samples split of {min_samples_split} and duration of {timer} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {\n",
    "    'Student':[],\n",
    "    'Agent':[]       \n",
    "} \n",
    "blocking_rates = {\n",
    "    'Student':[],\n",
    "    'Agent':[]  \n",
    "} \n",
    "student_accuracy = {\n",
    "    'Student':[],\n",
    "    'Agent':[] \n",
    "}\n",
    "\n",
    "start = t.time()\n",
    "env, expert = get_EnvExpert(top_name,k_path, alg_name, mean_service_holding_time=10)\n",
    "Agent_reward, _, Agent_df = evaluate_policy(env, n_eval_episodes, model = expert, return_dataframe=True)\n",
    "end = t.time()\n",
    "timer = end - start\n",
    "timer = round(timer/60,2)\n",
    "\n",
    "print(f\"mean reward agent is {Agent_reward} calculated with duration of {timer} minutes\")\n",
    "\n",
    "for sp in min_samples_splits:  \n",
    "    for key in rewards:\n",
    "        if key == 'Student':\n",
    "            start = t.time()\n",
    "            policy = DAgger_Policy.load(f'./tmp_DTop/cost239_{k_path}_{sp}/model.h5',env.observation_space,env.action_space)\n",
    "            mean_reward, _, df = evaluate_policy(env, n_eval_episodes, model = policy, return_dataframe=True)\n",
    "            end = t.time()\n",
    "            timer = end - start\n",
    "            timer = round(timer/60,2)\n",
    "            accuracy = 100*(1 - (abs(Agent_reward - mean_reward)/Agent_reward))\n",
    "        elif key == 'Agent':\n",
    "            mean_reward = Agent_reward\n",
    "            df = Agent_df\n",
    "            accuracy = 100\n",
    "            timer = 0\n",
    "        else:\n",
    "            raise Exception(\"\\n\\nSorry, key not found\")\n",
    "\n",
    "        rewards[key].append(mean_reward)\n",
    "        student_accuracy[key].append(accuracy)\n",
    "        blocking_rates[key].append(df['service_blocking_rate'][len(df['service_blocking_rate'])-1])\n",
    "        \n",
    "        \n",
    "        print(f'Done for {key} with expert mean_reward = {mean_reward} with duration of {timer} minutes and accuracy of {accuracy}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./plots_DT_min_samples_split/', exist_ok=True)\n",
    "\n",
    "df_r = pd.DataFrame(rewards)\n",
    "df_r.to_csv(f'./plots_DT_min_samples_split/rewards_{k_path}_sp.csv')\n",
    "df_r = pd.DataFrame(blocking_rates)\n",
    "df_r.to_csv(f'./plots_DT_min_samples_split/blocking_rates_{k_path}_sp.csv')\n",
    "df_r = pd.DataFrame(student_accuracy)\n",
    "df_r.to_csv(f'./plots_DT_min_samples_split/student_accuracy_{k_path}_sp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = './plots_DT_min_samples_split/blocking_rate/'\n",
    "colors = sns.color_palette(\"colorblind\")\n",
    "\n",
    "output_dir = base_output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.figure()\n",
    "\n",
    "for i, key in enumerate(rewards):\n",
    "    plt.plot(min_samples_splits, blocking_rates[key], 'x-', label=key, color=colors[i])\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"blocking rate\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.yticks(ticks=[5*10e-4, 10e-3,5*10e-3,10e-2,5*10e-2],labels=[ 5*10e-4, 10e-3,5*10e-3,10e-2,5*10e-2])\n",
    "# plt.yticks(ticks=[x/100 for x in range(1, 25,5)],labels=[x/100 for x in range(1, 25,5)])\n",
    "# plt.savefig(output_dir + f'{top_name}_{k_path}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = './plots_DT_min_samples_split/rewards/'\n",
    "\n",
    "output_dir = base_output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.figure()\n",
    "\n",
    "for i, key in enumerate(rewards):\n",
    "    plt.plot(min_samples_splits, rewards[key], 'x-', label=key, color=colors[i])\n",
    "plt.xlabel(\"max depths\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "# plt.savefig(output_dir + f'{top_name}_{k_path}.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
