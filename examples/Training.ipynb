{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical RL-Gym\n",
    "\n",
    "## Training the Stable Baselines agents using the DeepRLSA environment\n",
    "\n",
    "This file contains examples of how to train agents for the DeepRMSA environment.\n",
    "\n",
    "The agents used in this file come from the [Stable baselines](https://github.com/hill-a/stable-baselines) framework.\n",
    "\n",
    "This notebook is based upon the one available [here](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/master/monitor_training.ipynb).\n",
    "\n",
    "Before running this notebook, make sure to install Stable Baselines and the Optical RL-Gym in your Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# silencing tensorflow warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "tf.__version__ # printint out tensorflow version used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Baseline imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanha\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy \n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "stable_baselines3.__version__ # printing out stable_baselines version used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment imports\n",
    "\n",
    "In this particular example, there is no need to import anything specific to the Optical RL-Gym. Only by importing the Open AI Gym below, you already get all the functionality needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                 # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {} - \".format(self.num_timesteps),end=\"\")\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "                  # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                        self.model.save(self.save_path)\n",
    "                if self.verbose > 0:\n",
    "                    clear_output(wait=True)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "The parameters are set as in the [DeepRMSA](https://doi.org/10.1109/JLT.2019.2923615) work and its [available reporitory](https://github.com/xiaoliangchenUCD/DeepRMSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 200000 - Best mean reward: 22.17 - Last mean reward per episode: 20.71\n"
     ]
    }
   ],
   "source": [
    "alg_name = 'DQN'\n",
    "top_name = 'cost239'\n",
    "k_path = 10\n",
    "\n",
    "topology_dir = '/topologies/demo/' +  top_name +f'_{k_path}.h5'\n",
    "with open(f'..{topology_dir}', 'rb') as f:\n",
    "    topology = pickle.load(f)\n",
    "\n",
    "node_request_probabilities = np.array([1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11])\n",
    "\n",
    "for ht in [50]:\n",
    "    env_args = dict(topology=topology, seed=10, \n",
    "                    allow_rejection=False, # the agent cannot proactively reject a request\n",
    "                    j=1, # consider only the first suitable spectrum block for the spectrum assignment\n",
    "                    mean_service_holding_time=ht, # value is not set as in the paper to achieve comparable reward values\n",
    "                    episode_length=50, node_request_probabilities=node_request_probabilities, num_spectrum_resources = 358)\n",
    "\n",
    "    # Create log dir\n",
    "    log_dir = \"./tmp/deeprmsa-dqn-sbpp-agent-{}-cost239/\".format(ht)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=50, log_dir=log_dir)\n",
    "\n",
    "    env = gym.make('DeepRMSA-v0', **env_args)\n",
    "\n",
    "    # logs will be saved in log_dir/monitor.csv\n",
    "    # in this case, on top of the usual monitored things, we also monitor service and bit rate blocking probabilities\n",
    "    env = Monitor(env, log_dir + 'training', info_keywords=('episode_service_blocking_rate','bit_rate_blocking_rate','failure', 'episode_failure',\n",
    "                        'failure_slots','episode_failure_slots', \n",
    "                        'failure_disjointness','episode_failure_disjointness', 'failure_shared_disjointness',\n",
    "                        'episode_failure_shared_disjointness','shared_counter','episode_shared_counter', 'dpp_counter',\n",
    "                        'episode_dpp_counter','compactness', 'throughput', 'available_slots_working', 'available_slots_backup'))\n",
    "\n",
    "    # kwargs = {'double_q': True, 'prioritized_replay': True, 'policy_kwargs': dict(dueling=True)} # set of parameters for testing\n",
    "    policy_kwargs = dict(net_arch=4*[128])  \n",
    "\n",
    "    model = DQN(MlpPolicy, env, verbose=0, tensorboard_log=\"./tb/DQN-sbpp-agent-{}-cost239-DeepRMSA-v0/\".format(ht), gamma=0.85, policy_kwargs=policy_kwargs,\n",
    "            learning_rate=0.0001, exploration_fraction=0.01, batch_size = 256, buffer_size = 50000)\n",
    "\n",
    "    training_m = model.learn(total_timesteps=200000, callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000 - Best mean reward: 69.00 - Last mean reward per episode: 12.25\n"
     ]
    }
   ],
   "source": [
    "alg_name = 'DQN'\n",
    "top_name = 'cost239'\n",
    "k_path = 10\n",
    "\n",
    "topology_dir = '/topologies/demo/' +  top_name +f'_{k_path}.h5'\n",
    "with open(f'..{topology_dir}', 'rb') as f:\n",
    "    topology = pickle.load(f)\n",
    "\n",
    "node_request_probabilities = np.array([1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11])\n",
    "\n",
    "for ht in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "    env_args = dict(topology=topology, seed=10, \n",
    "                    allow_rejection=False, # the agent cannot proactively reject a request\n",
    "                    j=1, # consider only the first suitable spectrum block for the spectrum assignment\n",
    "                    mean_service_holding_time=ht, # value is not set as in the paper to achieve comparable reward values\n",
    "                    episode_length=50, node_request_probabilities=node_request_probabilities, num_spectrum_resources = 358)\n",
    "\n",
    "    # Create log dir\n",
    "    log_dir = \"./tmp/deeprmsa-dqn-sbpp-heuristic-{}-cost239/\".format(ht)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=50, log_dir=log_dir)\n",
    "\n",
    "    env = gym.make('DeepRMSAKSP-v0', **env_args)\n",
    "\n",
    "    # logs will be saved in log_dir/monitor.csv\n",
    "    # in this case, on top of the usual monitored things, we also monitor service and bit rate blocking probabilities\n",
    "    env = Monitor(env, log_dir + 'training', info_keywords=('episode_service_blocking_rate','bit_rate_blocking_rate','failure', 'episode_failure',\n",
    "                        'failure_slots','episode_failure_slots', \n",
    "                        'failure_disjointness','episode_failure_disjointness', 'failure_shared_disjointness',\n",
    "                        'episode_failure_shared_disjointness','shared_counter','episode_shared_counter', 'dpp_counter',\n",
    "                        'episode_dpp_counter','compactness', 'throughput', 'available_slots_working', 'available_slots_backup'))\n",
    "\n",
    "    # kwargs = {'double_q': True, 'prioritized_replay': True, 'policy_kwargs': dict(dueling=True)} # set of parameters for testing\n",
    "    policy_kwargs = dict(net_arch=4*[128])  \n",
    "\n",
    "    model = DQN(MlpPolicy, env, verbose=0, tensorboard_log=\"./tb/DQN-sbpp-heuristic-{}-cost239-DeepRMSAKSP-v0/\".format(ht), gamma=0.85, policy_kwargs=policy_kwargs,\n",
    "            learning_rate=0.0001, exploration_fraction=0.01, batch_size = 256, buffer_size = 50000)\n",
    "\n",
    "    training_m = model.learn(total_timesteps=1000, callback=callback)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
