{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "import pandas as pd\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as MLP_PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy as MLP_DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from explainable.utils import evaluate_policy\n",
    "#from explainable.envs.deeprmsa_env import shortest_available_path_first_fit\n",
    "from explainable.dagger import DAgger_Policy\n",
    "stable_baselines3.__version__ # printing out stable_baselines version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EnvExpert(top_name, k, alg_name, base_log_dir='./tmp/', only_spectrum_obs = False, mean_service_holding_time=10):\n",
    "    topology_dir = '/topologies/demo/' +  top_name +f'_{k}.h5'\n",
    "    with open(f'..{topology_dir}', 'rb') as f:\n",
    "        topology = pickle.load(f)\n",
    "    assert k <= topology.graph['k_paths']\n",
    "    \n",
    "    node_request_probabilities = np.array([1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11])\n",
    "\n",
    "    env_args = dict(topology=topology, seed=10, \n",
    "                    allow_rejection=False, # the agent cannot proactively reject a request\n",
    "                    j=1, # consider only the first suitable spectrum block for the spectrum assignment\n",
    "                    mean_service_holding_time=mean_service_holding_time, # value is not set as in the paper to achieve comparable reward values\n",
    "                    episode_length=50, node_request_probabilities=node_request_probabilities, num_spectrum_resources = 358)\n",
    "\n",
    "    # Create log dir\n",
    "    log_dir = \"./tmp/deeprmsa-dqn-sbpp-agent-{}-cost239/\".format(mean_service_holding_time)\n",
    "    env = gym.make('DeepRMSA-v0', **env_args)\n",
    "\n",
    "    # logs will be saved in log_dir/monitor.csv\n",
    "    # in this case, on top of the usual monitored things, we also monitor service and bit rate blocking probabilities\n",
    "    env = Monitor(env, log_dir + 'testing', info_keywords=('episode_service_blocking_rate','bit_rate_blocking_rate','failure', 'episode_failure',\n",
    "                        'failure_slots','episode_failure_slots', \n",
    "                        'failure_disjointness','episode_failure_disjointness', 'failure_shared_disjointness',\n",
    "                        'episode_failure_shared_disjointness','shared_counter','episode_shared_counter', 'dpp_counter',\n",
    "                        'episode_dpp_counter','compactness', 'throughput', 'available_slots_working', 'available_slots_backup'))\n",
    "    \n",
    "    expert = DQN.load(log_dir +'best_model')\n",
    "        \n",
    "    return env, expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EnvExpert_Heuristic(top_name, k, alg_name, base_log_dir='./tmp/', only_spectrum_obs = False, mean_service_holding_time=10):\n",
    "    topology_dir = '/topologies/demo/' +  top_name +f'_{k}.h5'\n",
    "    with open(f'..{topology_dir}', 'rb') as f:\n",
    "        topology = pickle.load(f)\n",
    "    assert k <= topology.graph['k_paths']\n",
    "    \n",
    "    node_request_probabilities = np.array([1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11, 1/11,\n",
    "                                       1/11, 1/11, 1/11])\n",
    "\n",
    "    env_args = dict(topology=topology, seed=10, \n",
    "                    allow_rejection=False, # the agent cannot proactively reject a request\n",
    "                    j=1, # consider only the first suitable spectrum block for the spectrum assignment\n",
    "                    mean_service_holding_time=mean_service_holding_time, # value is not set as in the paper to achieve comparable reward values\n",
    "                    episode_length=50, node_request_probabilities=node_request_probabilities, num_spectrum_resources = 358)\n",
    "\n",
    "    # Create log dir\n",
    "    log_dir = \"./tmp/deeprmsa-dqn-sbpp-heuristic-{}-cost239/\".format(mean_service_holding_time)\n",
    "    env = gym.make('DeepRMSAKSP-v0', **env_args)\n",
    "\n",
    "    # logs will be saved in log_dir/monitor.csv\n",
    "    # in this case, on top of the usual monitored things, we also monitor service and bit rate blocking probabilities\n",
    "    env = Monitor(env, log_dir + 'testing', info_keywords=('episode_service_blocking_rate','bit_rate_blocking_rate','failure', 'episode_failure',\n",
    "                        'failure_slots','episode_failure_slots', \n",
    "                        'failure_disjointness','episode_failure_disjointness', 'failure_shared_disjointness',\n",
    "                        'episode_failure_shared_disjointness','shared_counter','episode_shared_counter', 'dpp_counter',\n",
    "                        'episode_dpp_counter','compactness', 'throughput', 'available_slots_working', 'available_slots_backup'))\n",
    "    \n",
    "    expert = DQN.load(log_dir +'best_model')\n",
    "        \n",
    "    return env, expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = 'DQN'\n",
    "top_name = 'cost239'\n",
    "k_path = 10\n",
    "traffics = [100, 200, 300, 400, 500]\n",
    "holding_time = [10, 20, 30, 40, 50]\n",
    "n_eval_episodes = 2000\n",
    "use_heuristic_trainer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a DT with RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from explainable.dagger import DAgger\n",
    "from explainable.utils import collect_transitions\n",
    "\n",
    "for ht in holding_time: \n",
    "    env, expert = get_EnvExpert(top_name, k_path, alg_name, mean_service_holding_time=ht)\n",
    "\n",
    "    ### Collecting Expert Demostrations:\n",
    "    demostrations = collect_transitions(expert, env, 20000)\n",
    "\n",
    "    # Decision Trees Dagger Trainer:\n",
    "    tree_regr = tree.DecisionTreeClassifier(max_depth=15) # depth is set only for visualization purposes\n",
    "    tree_dagger = DAgger(expert, tree_regr, env, demostrations.copy(), max_depth=15, min_samples_split = 1000)\n",
    "\n",
    "    # Training the student policy:\n",
    "    tree_dagger.train(expert, env)\n",
    "\n",
    "    # saving the student policy:\n",
    "    save_dir = \"./tmp_students/\" + f'cost239_{k_path}_{ht}/'\n",
    "    tree_dagger.policy.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from explainable.dagger import DAgger\n",
    "from explainable.utils import collect_transitions\n",
    "\n",
    "for ht in holding_time: \n",
    "    env_heuristic, expert_heuristic = get_EnvExpert_Heuristic(top_name, k_path, alg_name, mean_service_holding_time=ht)\n",
    "\n",
    "    ### Collecting Expert Demostrations:\n",
    "    demostrations = collect_transitions(expert_heuristic, env_heuristic, 20000)\n",
    "\n",
    "    # Decision Trees Dagger Trainer:\n",
    "    tree_regr = tree.DecisionTreeClassifier(max_depth=15) # depth is set only for visualization purposes\n",
    "    tree_dagger = DAgger(expert_heuristic, tree_regr, env_heuristic, demostrations.copy(), max_depth=15, min_samples_split = 6000)\n",
    "\n",
    "    # Training the student policy:\n",
    "    tree_dagger.train(expert_heuristic, env_heuristic)\n",
    "\n",
    "    # saving the student policy:\n",
    "    save_dir = \"./tmp_students/\" + f'cost239_heuristic_{k_path}_{ht}/'\n",
    "    tree_dagger.policy.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "rewards = {\n",
    "    'Student':[],\n",
    "    #'Agent':[],\n",
    "    'KSPFF Student':[],\n",
    "    #'KSPFF':[]      \n",
    "} \n",
    "blocking_rates = {\n",
    "    'Student':[],\n",
    "    #'Agent':[],\n",
    "    'KSPFF Student':[],\n",
    "    #'KSPFF':[]    \n",
    "} \n",
    "failure_slots = {\n",
    "    'Student':[],\n",
    "    #'Agent':[],\n",
    "    'KSPFF Student':[],\n",
    "    #'KSPFF':[]    \n",
    "} \n",
    "failure_disjointness = {\n",
    "    'Student':[],\n",
    "    #'Agent':[],\n",
    "    'KSPFF Student':[],\n",
    "    #'KSPFF':[]   \n",
    "} \n",
    "evaluation_time = {\n",
    "    'Student':[],\n",
    "    #'Agent':[],\n",
    "    'KSPFF Student':[],\n",
    "    #'KSPFF':[]  \n",
    "}\n",
    "\n",
    "\n",
    "for ht in holding_time:  \n",
    "    for key in rewards:\n",
    "        if key == 'Student':\n",
    "            env, expert = get_EnvExpert(top_name, k_path, alg_name, mean_service_holding_time=ht)\n",
    "            policy = DAgger_Policy.load(f'./tmp_students/cost239_{k_path}_{ht}/model.h5',env.observation_space,env.action_space)\n",
    "            start = t.time()\n",
    "            mean_reward, _, df = evaluate_policy(env, n_eval_episodes, model = policy, return_dataframe=True)\n",
    "            end = t.time()\n",
    "            timer = end - start\n",
    "            timer = round(timer/60,2)\n",
    "        elif key == 'Agent':\n",
    "            env, expert = get_EnvExpert(top_name,k_path, alg_name, mean_service_holding_time=ht)\n",
    "            start = t.time()\n",
    "            mean_reward, _, df = evaluate_policy(env, n_eval_episodes, model = expert, return_dataframe=True)\n",
    "            end = t.time()\n",
    "            timer = end - start\n",
    "            timer = round(timer/60,2)\n",
    "        elif key == 'KSPFF Student':\n",
    "            env, expert = get_EnvExpert_Heuristic(top_name, k_path, alg_name, mean_service_holding_time=ht)\n",
    "            policy = DAgger_Policy.load(f'./tmp_students/cost239_heuristic_{k_path}_{ht}/model.h5',env.observation_space,env.action_space)\n",
    "            start = t.time()\n",
    "            mean_reward, _, df = evaluate_policy(env, n_eval_episodes, model = policy, return_dataframe=True)\n",
    "            end = t.time()\n",
    "            timer = end - start\n",
    "            timer = round(timer/60,2)\n",
    "        elif key == 'KSPFF':\n",
    "            env, expert = get_EnvExpert_Heuristic(top_name,k_path, alg_name, mean_service_holding_time=ht)\n",
    "            start = t.time()\n",
    "            mean_reward, _, df = evaluate_policy(env, n_eval_episodes, model = expert, return_dataframe=True)\n",
    "            end = t.time()\n",
    "            timer = end - start\n",
    "            timer = round(timer/60,2)\n",
    "        else:\n",
    "            raise Exception(\"\\n\\nSorry, key not found\")\n",
    "\n",
    "        \n",
    "        evaluation_time[key].append(timer)\n",
    "        rewards[key].append(mean_reward)\n",
    "        blocking_rates[key].append(df['service_blocking_rate'][len(df['service_blocking_rate'])-1])\n",
    "        \n",
    "        print(f'Done for {key} with expert mean_reward = {mean_reward} with duration of {timer} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./plots_students/', exist_ok=True)\n",
    "\n",
    "df_r = pd.DataFrame(rewards)\n",
    "df_r.to_csv(f'./plots_students/rewards_{k_path}.csv')\n",
    "df_r = pd.DataFrame(blocking_rates)\n",
    "df_r.to_csv(f'./plots_students/blocking_rates_{k_path}.csv')\n",
    "df_r = pd.DataFrame(evaluation_time)\n",
    "df_r.to_csv(f'./plots_students/evaluation_times_{k_path}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blocking rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = './plots_students/blocking_rate/'\n",
    "colors = sns.color_palette(\"colorblind\")\n",
    "\n",
    "output_dir = base_output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.figure()\n",
    "\n",
    "for i, key in enumerate(rewards):\n",
    "    plt.plot(traffics, blocking_rates[key], label=key, color=colors[i])\n",
    "plt.xlabel(\"traffic\")\n",
    "plt.ylabel(\"blocking rate\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.yticks(ticks=[5*10e-4, 10e-3,5*10e-3,10e-2,5*10e-2],labels=[ 5*10e-4, 10e-3,5*10e-3,10e-2,5*10e-2])\n",
    "# plt.yticks(ticks=[x/100 for x in range(1, 25,5)],labels=[x/100 for x in range(1, 25,5)])\n",
    "# plt.savefig(output_dir + f'{top_name}_{k_path}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = './plots_students/rewards/'\n",
    "\n",
    "output_dir = base_output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.figure()\n",
    "\n",
    "for i, key in enumerate(rewards):\n",
    "    plt.plot(traffics, rewards[key], label=key, color=colors[i])\n",
    "plt.xlabel(\"traffic\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "# plt.savefig(output_dir + f'{top_name}_{k_path}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features importance for the student trained with agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir_features = './plots_students/features_importance/'\n",
    "\n",
    "# loading saved DT classifier:\n",
    "#for ht in holding_time: \n",
    "env, expert = get_EnvExpert(top_name, k_path, alg_name, mean_service_holding_time=10)\n",
    "policy_agent = DAgger_Policy.load(f'./tmp_students/cost239_{k_path}_10/model.h5',env.observation_space,env.action_space)\n",
    "\n",
    "output_dir = base_output_dir_features\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "importances_sk = policy_agent.student.feature_importances_\n",
    "importance = pd.DataFrame(importances_sk)\n",
    "importance.to_csv(f'./plots_students/features_importance_{k_path}.csv')\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([x for x in range(len(importances_sk))], importances_sk)\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"importance\")\n",
    "plt.title(f\"Agent Features importance {top_name}\")\n",
    "# plt.savefig(output_dir + f'{top_name}_{k_path}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir_features = './plots_students/features_importance/'\n",
    "\n",
    "# loading saved DT classifier:\n",
    "#for ht in holding_time: \n",
    "env, expert = get_EnvExpert_Heuristic(top_name, k_path, alg_name, mean_service_holding_time=10)\n",
    "policy_kspff = DAgger_Policy.load(f'./tmp_students/cost239_heuristic_{k_path}_10/model.h5',env.observation_space,env.action_space)\n",
    "\n",
    "output_dir = base_output_dir_features\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "importances_sk = policy_kspff.student.feature_importances_\n",
    "importance = pd.DataFrame(importances_sk)\n",
    "importance.to_csv(f'./plots_students/features_importance_heuristic_{k_path}.csv')\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([x for x in range(len(importances_sk))], importances_sk)\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"importance\")\n",
    "plt.title(f\"KSPFF Features importance {top_name}\")\n",
    "# plt.savefig(output_dir + f'{top_name}_heuristic_{k_path}}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "dot_data = tree.export_graphviz(policy_agent.student, out_file=None, filled=True, rotate=True, max_depth=3)\n",
    "graph = graphviz.Source(dot_data, format=\"png\", directory='./plots_students/DTClassifier/')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.render(\"DT Classifier DRL Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "dot_data = tree.export_graphviz(policy_kspff.student, out_file=None, filled=True, rotate=True, max_depth=4)\n",
    "graph = graphviz.Source(dot_data, format=\"png\", directory='./plots_students/DTClassifier/')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.render(\"DT Classifier KSPFF Heuristics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
